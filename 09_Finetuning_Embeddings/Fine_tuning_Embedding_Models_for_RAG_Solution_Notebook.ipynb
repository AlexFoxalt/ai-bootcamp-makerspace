{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckbbj5diaHkg"
   },
   "source": [
    "# Fine-tuning Embeddings for RAG on Specific Data\n",
    "\n",
    "As we start our \"fine-tuning\" week, we'll start with the lowest hanging improvement one can do for RAG - which is:\n",
    "\n",
    "Fine-tuning embeddings!\n",
    "\n",
    "- ðŸ¤ Breakout Room #1:\n",
    "  - Task 1: Dependencies and Boilerplate\n",
    "  - Task 2: Loading Data\n",
    "  - Task 3: Constructing a Fine-tuning Dataset\n",
    "  - Task 4: Fine-tuning `snowflake-arctic-embed-l`\n",
    "  - Task 5: Evaluating our Retriever\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xwor_3X6ODX"
   },
   "source": [
    "#### Basic Overview of Fine-tuning Embeddings\n",
    "\n",
    "In essence, what we want to do when we fine-tune our embedding models is very simple:\n",
    "\n",
    "```\n",
    "Move the embeddings for questions relating to a document\n",
    "closer together with that document\n",
    "```\n",
    "\n",
    "We can think of fine-tuning our embedding models as follows:\n",
    "\n",
    "1) We have some pair of text items that *should* be closer together\n",
    "  - `Question`, `Document` pairs\n",
    "  - EX: `Who drives the bus?`, `The bus was driven by Kyle, the Bus Driver`.\n",
    "\n",
    "2) We use these pairs as labeled data to fine-tune our embedding model.\n",
    "\n",
    "The process of training helps the model more accurately associate our questions with the correct documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DX5R3HVz6FOQ"
   },
   "source": [
    "#####â“ Question #1:\n",
    "\n",
    "Describe the nuance between using Q&D pairs to train the embedding model vs. inter-document pairs/related sentences.\n",
    "\n",
    "What caveats does this approach have? Are there any special considerations for what kind of Q's we should use?\n",
    "\n",
    "---\n",
    "\n",
    "**ANSWER:**\n",
    "\n",
    "We are specifically relating *the questions* to *the documents*. This means that we are making our embedding model at the very specific task of relating potential questions to specific documents.\n",
    "\n",
    "There are many caveats, but the main ones are:\n",
    "\n",
    "- Your Q's should reflect the Q's of your users\n",
    "- This kind of fine-tuning will (purposefully) \"overfit\" on your data; this is the desired result in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NkSaurzbpyS"
   },
   "source": [
    "## Task 1: Dependencies and Boilerplate\n",
    "\n",
    "We'll set up our `nest_asyncio` so we can leverage async loops in our Notebook.\n",
    "\n",
    "We'll also install the required libraries we'll be using today, and set up our OpenAI API key!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9c_EUibmcDU3"
   },
   "source": [
    "### Nest Asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "zq-6s7LbPnKH"
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ulZIBA1ZoSsV",
    "outputId": "12d9c766-843f-40bf-bdf8-e0ed04b6d87f"
   },
   "outputs": [],
   "source": [
    "#!pip install -qU langchain_openai langchain_huggingface langchain_core langchain langchain_community langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3GFD7B-tOCrx"
   },
   "outputs": [],
   "source": [
    "#!pip install -qU faiss-cpu python-pptx==1.0.2 nltk==3.9.1 pymupdf beautifulsoup4 lxml "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FM-eUlrcI8a"
   },
   "source": [
    "### Provide OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wA_mlurVqtrp",
    "outputId": "18cccb1e-095f-40fa-def5-2454f9bcdcae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFZ217gCDVTr"
   },
   "source": [
    "## Task 2: Loading Data\n",
    "\n",
    "We'll prepare our data - and download our webpages which we'll be using for our data today.\n",
    "\n",
    "These webpages are from [Simon Willison's](https://simonwillison.net/) yearly \"AI learnings\".\n",
    "\n",
    "- [2023 Blog](https://simonwillison.net/2023/Dec/31/ai-in-2023/)\n",
    "- [2024 Blog](https://simonwillison.net/2024/Dec/31/llms-in-2024/)\n",
    "\n",
    "Let's start by collecting our data into a useful pile!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 31440    0 31440    0     0   149k      0 --:--:-- --:--:-- --:--:--  149k\n"
     ]
    }
   ],
   "source": [
    "!curl https://simonwillison.net/2023/Dec/31/ai-in-2023/ -o data/2023_llms.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 70299    0 70299    0     0   525k      0 --:--:-- --:--:-- --:--:--  528k\n"
     ]
    }
   ],
   "source": [
    "!curl https://simonwillison.net/2024/Dec/31/llms-in-2024/ -o data/2024_llms.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "DHJhTzsvN75t"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "\n",
    "path = \"data/\"\n",
    "text_loader = DirectoryLoader(path, glob=\"*.html\", loader_cls=BSHTMLLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UbKa6-V0nvp"
   },
   "source": [
    "Next, we'll set up a classic naive chunking strategy as we only care that the documents get parsed into chunks that we can generate synthetic questions about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NsPrOOqXOsNX"
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 750,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = len\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lf_PoX7l09Rg"
   },
   "source": [
    "Next we can load/split these documents as follows.\n",
    "\n",
    ">> NOTE: You may need to run this cell twice to get it to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "OMYPX6N6Os8M"
   },
   "outputs": [],
   "source": [
    "training_documents = text_splitter.split_documents(text_loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PAozuMoNOvnp",
    "outputId": "dc1d663e-7153-4c51-cedb-d1bc3888c4ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yE2TFIq1BuJ"
   },
   "source": [
    "Next, we're going to associate each of our chunks with a unique identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "AwyIForybIpo"
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "id_set = set()\n",
    "\n",
    "for document in training_documents:\n",
    "  id = str(uuid.uuid4())\n",
    "  while id in id_set:\n",
    "    id = uuid.uuid4()\n",
    "  id_set.add(id)\n",
    "  document.metadata[\"id\"] = id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJnL4oNg341U"
   },
   "source": [
    "Next, we'll simply use naive Python slicing to create a training, test, and validation set to prepare our data for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "MTS4GTSEcnG4"
   },
   "outputs": [],
   "source": [
    "documents_total = len(training_documents)\n",
    "training_split_documents = training_documents[:documents_total - 24]  # the biggest chunk for training\n",
    "validation_split_documents = training_documents[documents_total - 24:documents_total-12]  # small chunks by 12 elems for test and validation\n",
    "test_split_documents = training_documents[documents_total - 12:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78, 12, 12)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_split_documents), len(validation_split_documents), len(test_split_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzlvKbONDWvQ"
   },
   "source": [
    "## Task 3: Constructing a Fine-tuning Dataset\n",
    "\n",
    "Using the nodes we created above, we can finally start constructing a fine-tuning dataset utilizing OpenAI's `gpt-4o-mini` (released [today](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/)).\n",
    "\n",
    "The basic idea here is straightforward enough:\n",
    "\n",
    "1. We look at a document\n",
    "2. We generate questions that could be answered by that node\n",
    "\n",
    "This gives us a number of question/context pairs that we can use to fine-tune our Embeddings model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_EWfmIscMrvg"
   },
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[0;32m----> 3\u001b[0m qa_chat_model \u001b[38;5;241m=\u001b[39m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4o-mini\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/ai_bootcamp/ai-bootcamp-makerspace/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/PycharmProjects/ai_bootcamp/ai-bootcamp-makerspace/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:622\u001b[0m, in \u001b[0;36mBaseChatOpenAI.validate_environment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client \u001b[38;5;241m=\u001b[39m httpx\u001b[38;5;241m.\u001b[39mClient(proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopenai_proxy)\n\u001b[1;32m    621\u001b[0m     sync_specific \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client}\n\u001b[0;32m--> 622\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mclient_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msync_specific\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client:\n",
      "File \u001b[0;32m~/PycharmProjects/ai_bootcamp/ai-bootcamp-makerspace/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/openai/_client.py:110\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    108\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "qa_chat_model = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-hLnsSB6Y-S"
   },
   "source": [
    "We'll create a simple Question Generation prompt to query `gpt-4o-mini` to generate Questions for each retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "diEWcw00NMSj"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "qa_prompt = \"\"\"\\\n",
    "Given the following context, you must generate questions based on only the provided context.\n",
    "\n",
    "You are to generate {n_questions} questions which should be provided in the following JSON format:\n",
    "{{\n",
    "\"questions\": [\"QUESTION #1\", \"QUESTION #2\", ...]\n",
    "}}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "qa_prompt_template = ChatPromptTemplate.from_template(qa_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u87Izpgm6_fk"
   },
   "source": [
    "We'll create a simple chain to query the LLM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ggl9SSjiNbpG"
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "question_generation_chain = qa_prompt_template | qa_chat_model | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4duvHirh7DQv"
   },
   "source": [
    "There's a lot going on in this function - let's take a deeper look:\n",
    "\n",
    "1. First, we provide a list of documents and a number of questions\n",
    "2. We, for each document in our list, generate `n_questions` of questions.\n",
    "3. We then associate those questions and contexts via a `UUID`.\n",
    "\n",
    "> NOTE: The reason we're doing this `UUID` association is for ease of use later in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Lm2JvgC9X37"
   },
   "source": [
    "##### ðŸ—ï¸ Activity #1:\n",
    "\n",
    "We have:\n",
    "\n",
    "- Lists of `Documents` with the `metadata` field `id`.\n",
    "\n",
    "We need:\n",
    "\n",
    "- An object with key `id`, which have values `str` questions.\n",
    "- An object with key `question_id`, which have values `List(str)` which will be a list of associated `context_id`.\n",
    "\n",
    "An Example:\n",
    "\n",
    "question_object:\n",
    "```python\n",
    "{\n",
    "'b4b95fb6-f827-4454-aa5b-20e62733f172': 'What types of accessible formats are available for persons with disabilities?',\n",
    "'df58ee4f-714c-419e-8324-94e5870574e2': 'How do accessible formats benefit persons with disabilities?',\n",
    "'505fce8b-0e56-48de-a251-61027e396918': 'What are some of the risks associated with the increasing capabilities of AI systems that generate synthetic content?',\n",
    "'8ff0ab33-60dc-4fee-8958-91bfb686aca8': 'Why is it important for providers of AI systems to embed technical solutions for marking and detecting synthetic content?'\n",
    "}\n",
    " ```\n",
    "\n",
    " context_object:\n",
    " ```python\n",
    "{\n",
    "'b4b95fb6-f827-4454-aa5b-20e62733f172': ['dd75bf94-75f3-4603-8e4b-5522f6925638'],\n",
    "'df58ee4f-714c-419e-8324-94e5870574e2': ['dd75bf94-75f3-4603-8e4b-5522f6925638'],\n",
    "'505fce8b-0e56-48de-a251-61027e396918': ['ffe3893f-688c-48e8-90bd-7a9feb953d90'],\n",
    "'8ff0ab33-60dc-4fee-8958-91bfb686aca8': ['ffe3893f-688c-48e8-90bd-7a9feb953d90'],\n",
    "}\n",
    " ```\n",
    "\n",
    " As you can see, a piece of context can be associated with more than 1 question.\n",
    "\n",
    " The task is to write the Python function(s) to accomplish this task.\n",
    "\n",
    " Your function signature is provided below, along with the desired return values.\n",
    "\n",
    " > NOTE: You can make any modifications that you desire - assuming that you have the correct input and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "U4yi4NfTCnLc"
   },
   "outputs": [],
   "source": [
    "from tqdm.asyncio import tqdm\n",
    "from langchain_core.documents import Document\n",
    "import uuid\n",
    "\n",
    "\n",
    "async def create_questions(documents: list[Document], n_questions: int = 3) -> tuple[dict]:\n",
    "  questions = {}\n",
    "  relevant_docs = {}\n",
    "\n",
    "  for doc in tqdm(documents, total=len(documents), desc=\"Processing docs\"):\n",
    "      llm_response = await question_generation_chain.ainvoke({\"context\": doc.page_content, \"n_questions\": n_questions})\n",
    "      for q in llm_response[\"questions\"]:\n",
    "          q_id = str(uuid.uuid4())\n",
    "          questions[q_id] = q\n",
    "          relevant_docs[q_id] = [doc.metadata[\"id\"]]\n",
    "\n",
    "  return questions, relevant_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5W0eWOUo4QGL"
   },
   "source": [
    "### REMOVE `await` IF NOT USING ASYNC (HINT: Use `async`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85Dq6KRqEs0F",
    "outputId": "60dcd580-b6e8-4e3b-d605-05b492ca5c96"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing docs:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                       | 41/78 [00:55<00:49,  1.35s/it]\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_questions, training_relevant_contexts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m create_questions(training_split_documents, \u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[30], line 11\u001b[0m, in \u001b[0;36mcreate_questions\u001b[0;34m(documents, n_questions)\u001b[0m\n\u001b[1;32m      8\u001b[0m relevant_docs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m tqdm(documents, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(documents), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing docs\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 11\u001b[0m     llm_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m question_generation_chain\u001b[38;5;241m.\u001b[39mainvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m: doc\u001b[38;5;241m.\u001b[39mpage_content, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_questions\u001b[39m\u001b[38;5;124m\"\u001b[39m: n_questions})\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m llm_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestions\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     13\u001b[0m         q_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(uuid\u001b[38;5;241m.\u001b[39muuid4())\n",
      "File \u001b[0;32m~/PycharmProjects/ai_bootcamp/ai-bootcamp-makerspace/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:3058\u001b[0m, in \u001b[0;36mRunnableSequence.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3056\u001b[0m     part \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(step\u001b[38;5;241m.\u001b[39mainvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m asyncio_accepts_context():\n\u001b[0;32m-> 3058\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(part(), context\u001b[38;5;241m=\u001b[39mcontext)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   3059\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3060\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(part())\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/futures.py:286\u001b[0m, in \u001b[0;36mFuture.__await__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_asyncio_future_blocking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mawait wasn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt used with future\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py:375\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 375\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    377\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[1;32m    378\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/futures.py:194\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the result this future represents.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03mIf the future has been cancelled, raises CancelledError.  If the\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03mfuture's result isn't yet available, raises InvalidStateError.  If\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03mthe future is done and has an exception set, this exception is raised.\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m _CANCELLED:\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_cancelled_error()\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m!=\u001b[39m _FINISHED:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mInvalidStateError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResult is not ready.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py:306\u001b[0m, in \u001b[0;36mTask.__step_run_and_handle_result\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    304\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 306\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_must_cancel:\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/ai_bootcamp/ai-bootcamp-makerspace/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:305\u001b[0m, in \u001b[0;36mBaseChatModel.ainvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mainvoke\u001b[39m(\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    303\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    304\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m--> 305\u001b[0m     llm_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate_prompt(\n\u001b[1;32m    306\u001b[0m         [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    307\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    308\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    309\u001b[0m         tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    310\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    311\u001b[0m         run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    312\u001b[0m         run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    314\u001b[0m     )\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ChatGeneration, llm_result\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/PycharmProjects/ai_bootcamp/ai-bootcamp-makerspace/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:870\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21magenerate_prompt\u001b[39m(\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    864\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    867\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    868\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    869\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 870\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate(\n\u001b[1;32m    871\u001b[0m         prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    872\u001b[0m     )\n",
      "File \u001b[0;32m~/PycharmProjects/ai_bootcamp/ai-bootcamp-makerspace/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:796\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    776\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m AsyncCallbackManager\u001b[38;5;241m.\u001b[39mconfigure(\n\u001b[1;32m    777\u001b[0m     callbacks,\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata,\n\u001b[1;32m    784\u001b[0m )\n\u001b[1;32m    786\u001b[0m run_managers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m callback_manager\u001b[38;5;241m.\u001b[39mon_chat_model_start(\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[1;32m    788\u001b[0m     messages,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    793\u001b[0m     run_id\u001b[38;5;241m=\u001b[39mrun_id,\n\u001b[1;32m    794\u001b[0m )\n\u001b[0;32m--> 796\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m    797\u001b[0m     \u001b[38;5;241m*\u001b[39m[\n\u001b[1;32m    798\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate_with_cache(\n\u001b[1;32m    799\u001b[0m             m,\n\u001b[1;32m    800\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    801\u001b[0m             run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    803\u001b[0m         )\n\u001b[1;32m    804\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages)\n\u001b[1;32m    805\u001b[0m     ],\n\u001b[1;32m    806\u001b[0m     return_exceptions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    807\u001b[0m )\n\u001b[1;32m    808\u001b[0m exceptions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    809\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, res \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results):\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py:375\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 375\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    377\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[1;32m    378\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py:304\u001b[0m, in \u001b[0;36mTask.__step_run_and_handle_result\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m~/PycharmProjects/ai_bootcamp/ai-bootcamp-makerspace/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:998\u001b[0m, in \u001b[0;36mBaseChatModel._agenerate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 998\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate(\n\u001b[1;32m    999\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1000\u001b[0m         )\n\u001b[1;32m   1001\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1002\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/PycharmProjects/ai_bootcamp/ai-bootcamp-makerspace/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:960\u001b[0m, in \u001b[0;36mBaseChatOpenAI._agenerate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response\u001b[38;5;241m.\u001b[39mheaders)}\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 960\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpayload)\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m run_in_executor(\n\u001b[1;32m    962\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result, response, generation_info\n\u001b[1;32m    963\u001b[0m )\n",
      "File \u001b[0;32m~/PycharmProjects/ai_bootcamp/ai-bootcamp-makerspace/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/openai/resources/chat/completions.py:1727\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1685\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1686\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   1687\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1724\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m   1725\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m AsyncStream[ChatCompletionChunk]:\n\u001b[1;32m   1726\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m-> 1727\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   1728\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1729\u001b[0m         body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[1;32m   1730\u001b[0m             {\n\u001b[1;32m   1731\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m   1732\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   1733\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[1;32m   1734\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   1735\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m   1736\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m   1737\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m   1738\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m   1739\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m   1740\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m   1741\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m   1742\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[1;32m   1743\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m   1744\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m   1745\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[1;32m   1746\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m   1747\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[1;32m   1748\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m   1749\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m   1750\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m   1751\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m   1752\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m   1753\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   1754\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m   1755\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   1756\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m   1757\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m   1758\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m   1759\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   1760\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m   1761\u001b[0m             },\n\u001b[1;32m   1762\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m   1763\u001b[0m         ),\n\u001b[1;32m   1764\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   1765\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   1766\u001b[0m         ),\n\u001b[1;32m   1767\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m   1768\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1769\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[1;32m   1770\u001b[0m     )\n",
      "File \u001b[0;32m~/PycharmProjects/ai_bootcamp/ai-bootcamp-makerspace/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/openai/_base_client.py:1849\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1835\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1836\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1837\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1844\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1845\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[1;32m   1846\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1847\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1848\u001b[0m     )\n\u001b[0;32m-> 1849\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m~/PycharmProjects/ai_bootcamp/ai-bootcamp-makerspace/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/openai/_base_client.py:1543\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1540\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1541\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1544\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1545\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1546\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1547\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1548\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1549\u001b[0m )\n",
      "File \u001b[0;32m~/PycharmProjects/ai_bootcamp/ai-bootcamp-makerspace/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/openai/_base_client.py:1582\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1579\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauth\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_auth\n\u001b[1;32m   1581\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1582\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(\n\u001b[1;32m   1583\u001b[0m         request,\n\u001b[1;32m   1584\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_stream_response_body(request\u001b[38;5;241m=\u001b[39mrequest),\n\u001b[1;32m   1585\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1586\u001b[0m     )\n\u001b[1;32m   1587\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   1588\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/PycharmProjects/ai_bootcamp/ai-bootcamp-makerspace/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/httpx/_client.py:1629\u001b[0m, in \u001b[0;36mAsyncClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m   1625\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m   1627\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m-> 1629\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_auth(\n\u001b[1;32m   1630\u001b[0m     request,\n\u001b[1;32m   1631\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[1;32m   1632\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m   1633\u001b[0m     history\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m   1634\u001b[0m )\n\u001b[1;32m   1635\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1636\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/PycharmProjects/ai_bootcamp/ai-bootcamp-makerspace/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/httpx/_client.py:1657\u001b[0m, in \u001b[0;36mAsyncClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1654\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m auth_flow\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__anext__\u001b[39m()\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1657\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_redirects(\n\u001b[1;32m   1658\u001b[0m         request,\n\u001b[1;32m   1659\u001b[0m         follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m   1660\u001b[0m         history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[1;32m   1661\u001b[0m     )\n\u001b[1;32m   1662\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1663\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/PycharmProjects/ai_bootcamp/ai-bootcamp-makerspace/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/httpx/_client.py:1694\u001b[0m, in \u001b[0;36mAsyncClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1691\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1692\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m hook(request)\n\u001b[0;32m-> 1694\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_single_request(request)\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1696\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/PycharmProjects/ai_bootcamp/ai-bootcamp-makerspace/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/httpx/_client.py:1730\u001b[0m, in \u001b[0;36mAsyncClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1725\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1726\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an sync request with an AsyncClient instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1727\u001b[0m     )\n\u001b[1;32m   1729\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1730\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m transport\u001b[38;5;241m.\u001b[39mhandle_async_request(request)\n\u001b[1;32m   1732\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, AsyncByteStream)\n\u001b[1;32m   1733\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/PycharmProjects/ai_bootcamp/ai-bootcamp-makerspace/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/httpx/_transports/default.py:394\u001b[0m, in \u001b[0;36mAsyncHTTPTransport.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    381\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    382\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    383\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    391\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    392\u001b[0m )\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 394\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_async_request(req)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mAsyncIterable)\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    399\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    400\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    401\u001b[0m     stream\u001b[38;5;241m=\u001b[39mAsyncResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    402\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    403\u001b[0m )\n",
      "File \u001b[0;32m~/PycharmProjects/ai_bootcamp/ai-bootcamp-makerspace/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/httpcore/_async/connection_pool.py:256\u001b[0m, in \u001b[0;36mAsyncConnectionPool.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mAsyncIterable)\n",
      "File \u001b[0;32m~/PycharmProjects/ai_bootcamp/ai-bootcamp-makerspace/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/httpcore/_async/connection_pool.py:236\u001b[0m, in \u001b[0;36mAsyncConnectionPool.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m connection\u001b[38;5;241m.\u001b[39mhandle_async_request(\n\u001b[1;32m    237\u001b[0m         pool_request\u001b[38;5;241m.\u001b[39mrequest\n\u001b[1;32m    238\u001b[0m     )\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/PycharmProjects/ai_bootcamp/ai-bootcamp-makerspace/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/httpcore/_async/connection.py:103\u001b[0m, in \u001b[0;36mAsyncHTTPConnection.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_async_request(request)\n",
      "File \u001b[0;32m~/PycharmProjects/ai_bootcamp/ai-bootcamp-makerspace/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/httpcore/_async/http11.py:136\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/PycharmProjects/ai_bootcamp/ai-bootcamp-makerspace/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/httpcore/_async/http11.py:106\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    100\u001b[0m     (\n\u001b[1;32m    101\u001b[0m         http_version,\n\u001b[1;32m    102\u001b[0m         status,\n\u001b[1;32m    103\u001b[0m         reason_phrase,\n\u001b[1;32m    104\u001b[0m         headers,\n\u001b[1;32m    105\u001b[0m         trailing_data,\n\u001b[0;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_response_headers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/PycharmProjects/ai_bootcamp/ai-bootcamp-makerspace/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/httpcore/_async/http11.py:177\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_event(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/ai_bootcamp/ai-bootcamp-makerspace/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/httpcore/_async/http11.py:217\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\u001b[38;5;241m.\u001b[39mread(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREAD_NUM_BYTES, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    219\u001b[0m     )\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/PycharmProjects/ai_bootcamp/ai-bootcamp-makerspace/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/httpcore/_backends/anyio.py:35\u001b[0m, in \u001b[0;36mAnyIOStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39mfail_after(timeout):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream\u001b[38;5;241m.\u001b[39mreceive(max_bytes\u001b[38;5;241m=\u001b[39mmax_bytes)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39mEndOfStream:  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/PycharmProjects/ai_bootcamp/ai-bootcamp-makerspace/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/anyio/streams/tls.py:204\u001b[0m, in \u001b[0;36mTLSStream.receive\u001b[0;34m(self, max_bytes)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreceive\u001b[39m(\u001b[38;5;28mself\u001b[39m, max_bytes: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m65536\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbytes\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_sslobject_method(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ssl_object\u001b[38;5;241m.\u001b[39mread, max_bytes)\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m EndOfStream\n",
      "File \u001b[0;32m~/PycharmProjects/ai_bootcamp/ai-bootcamp-makerspace/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/anyio/streams/tls.py:147\u001b[0m, in \u001b[0;36mTLSStream._call_sslobject_method\u001b[0;34m(self, func, *args)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_bio\u001b[38;5;241m.\u001b[39mpending:\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransport_stream\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_bio\u001b[38;5;241m.\u001b[39mread())\n\u001b[0;32m--> 147\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransport_stream\u001b[38;5;241m.\u001b[39mreceive()\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EndOfStream:\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_bio\u001b[38;5;241m.\u001b[39mwrite_eof()\n",
      "File \u001b[0;32m~/PycharmProjects/ai_bootcamp/ai-bootcamp-makerspace/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/anyio/_backends/_asyncio.py:1246\u001b[0m, in \u001b[0;36mSocketStream.receive\u001b[0;34m(self, max_bytes)\u001b[0m\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1241\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_protocol\u001b[38;5;241m.\u001b[39mread_event\u001b[38;5;241m.\u001b[39mis_set()\n\u001b[1;32m   1242\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39mis_closing()\n\u001b[1;32m   1243\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_protocol\u001b[38;5;241m.\u001b[39mis_at_eof\n\u001b[1;32m   1244\u001b[0m ):\n\u001b[1;32m   1245\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39mresume_reading()\n\u001b[0;32m-> 1246\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_protocol\u001b[38;5;241m.\u001b[39mread_event\u001b[38;5;241m.\u001b[39mwait()\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39mpause_reading()\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/locks.py:213\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_waiters\u001b[38;5;241m.\u001b[39mappend(fut)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/futures.py:286\u001b[0m, in \u001b[0;36mFuture.__await__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_asyncio_future_blocking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mawait wasn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt used with future\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_questions, training_relevant_contexts = await create_questions(training_split_documents, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FSTG0bb7w73"
   },
   "source": [
    "We'll use the function to generate training, validation, and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eIZm4CqGVzBx",
    "outputId": "65a7703a-c528-40f6-aff4-1be84902cfc4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing docs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:21<00:00,  1.77s/it]\n"
     ]
    }
   ],
   "source": [
    "val_questions, val_relevant_contexts = await create_questions(validation_split_documents, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o6qUHg9sV2_y",
    "outputId": "b03bf5c6-d392-40bf-a061-1daceba2962e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing docs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:15<00:00,  1.28s/it]\n"
     ]
    }
   ],
   "source": [
    "test_questions, test_relevant_contexts = await create_questions(test_split_documents, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_jYOnAI43zK"
   },
   "source": [
    "### Reformating and Saving Datasets\n",
    "\n",
    "Now, we can save our datasets for later use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "iF6IFFq9VsNu"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "training_corpus = {train_item.metadata[\"id\"] : train_item.page_content for train_item in training_split_documents}\n",
    "\n",
    "train_dataset = {\n",
    "    \"questions\" : training_questions,\n",
    "    \"relevant_contexts\" : training_relevant_contexts,\n",
    "    \"corpus\" : training_corpus\n",
    "}\n",
    "\n",
    "with open(\"training_dataset.jsonl\", \"w\") as f:\n",
    "  json.dump(train_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "PqF9WaueV-V8"
   },
   "outputs": [],
   "source": [
    "val_corpus = {val_item.metadata[\"id\"] : val_item.page_content for val_item in validation_split_documents}\n",
    "\n",
    "val_dataset = {\n",
    "    \"questions\" : val_questions,\n",
    "    \"relevant_contexts\" : val_relevant_contexts,\n",
    "    \"corpus\" : val_corpus\n",
    "}\n",
    "\n",
    "with open(\"val_dataset.jsonl\", \"w\") as f:\n",
    "  json.dump(val_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "0DSQ7WMnWAu6"
   },
   "outputs": [],
   "source": [
    "train_corpus = {test_item.metadata[\"id\"] : test_item.page_content for test_item in test_split_documents}\n",
    "\n",
    "test_dataset = {\n",
    "    \"questions\" : test_questions,\n",
    "    \"relevant_contexts\" : test_relevant_contexts,\n",
    "    \"corpus\" : train_corpus\n",
    "}\n",
    "\n",
    "with open(\"test_dataset.jsonl\", \"w\") as f:\n",
    "  json.dump(test_dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAwklqQCgVi-"
   },
   "source": [
    "## Task 4: Fine-tuning `snowflake-arctic-embed-l`\n",
    "\n",
    "Now that we have a dataset, let's grab a `sentence-transformers` Embeddings model!\n",
    "\n",
    "We'll be using Snowflake's [`snowflake-arctic-embed-l`](https://huggingface.co/Snowflake/snowflake-arctic-embed-l) as a base embeddings model.\n",
    "\n",
    "It is a well performing embeddings model by itself, but there's a lot of very specific domain terms and vocabulary in our courpus - so lets fine-tune it and see what that can do for us!\n",
    "\n",
    ">> NOTE: Skip installing dependencies if you are running this notebook locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AXzVHP3v1Cno",
    "outputId": "a9d6ca65-d355-460d-de89-7446a441512b"
   },
   "outputs": [],
   "source": [
    "#!pip install -qU sentence_transformers datasets pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G-PGsQB7Xo6V",
    "outputId": "8df58392-a82b-45f9-ce4e-b4155522e2c6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08140b8945804f10b71ce517363ed8b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc68420e146240f3826f2bacaa032800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/252 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "862b9562f9a04ae69729cb8ff6b22149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/85.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a221e2ec6554becac3844572eda321d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/107 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47ad9cb374a64189ad013062248a6637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/704 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "775a83dd622e4f8ba6964bb9d46f99f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "978ff0b9d2374662be7119278bb7e635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bfb1d39998e44cb99b86e8409e7cf01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa245b12536542b1b22d9346b40070da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b245667a563409890e528851fdf755c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c256bee4676f4d488b9911501868e4f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling%2Fconfig.json:   0%|          | 0.00/297 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_id = \"Snowflake/snowflake-arctic-embed-l\"\n",
    "model = SentenceTransformer(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ztG07iB8CFO"
   },
   "source": [
    "We'll grab some necessary imports from `sentence_transformers` and `torch`.\n",
    "\n",
    "> NOTE: PyTorch (`torch`) is a popular machine learning library - while we don't go very deep into PyTorch it's an incredibly powerful and interesting library! Please read more about it [here](https://pytorch.org/tutorials/beginner/basics/intro.html)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "B-WbpuUWYFJr"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from sentence_transformers import InputExample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJtPPlck8HBE"
   },
   "source": [
    "We're using a toy batch size here to reflect the limited number of examples we have.\n",
    "\n",
    "> NOTE: It is typical to use a much larger batch size (~64+), hardware permitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "8Lokhy6KYHAv"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-6DT8hc8PmT"
   },
   "source": [
    "Let's move our dataset into the expected format for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "JJk37zQsYJ4P"
   },
   "outputs": [],
   "source": [
    "corpus = train_dataset['corpus']\n",
    "queries = train_dataset['questions']\n",
    "relevant_docs = train_dataset['relevant_contexts']\n",
    "\n",
    "examples = []\n",
    "for query_id, query in queries.items():\n",
    "    doc_id = relevant_docs[query_id][0]\n",
    "    text = corpus[doc_id]\n",
    "    example = InputExample(texts=[query, text])\n",
    "    examples.append(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OjFx7KHI8TL0"
   },
   "source": [
    "Now we can create a `torch` `DataLoader`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "tiizmeIqZ_-w"
   },
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "    examples, batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vA8rzlX8XbT"
   },
   "source": [
    "Next up, we'll prepare our loss function!\n",
    "\n",
    "Loss is an important part of training, fine-tuning, and more. If you want a deep dive on loss - you can check out our [event on loss!](https://www.youtube.com/watch?v=iB8FWR9aD5Q&t=8s).\n",
    "\n",
    "The core loss we're using today is called `MultipleNegativesRankingLoss` - you can find more information [here](https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/losses/MultipleNegativesRankingLoss.py).\n",
    "\n",
    "This is \"wrapped\" in `MatryoshkaLoss`, which you can read the implementation of [here](https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/losses/MatryoshkaLoss.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "Uga4nnBqlVeh"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers.losses import MatryoshkaLoss, MultipleNegativesRankingLoss\n",
    "\n",
    "matryoshka_dimensions = [768, 512, 256, 128, 64]\n",
    "inner_train_loss = MultipleNegativesRankingLoss(model)\n",
    "train_loss = MatryoshkaLoss(\n",
    "    model, inner_train_loss, matryoshka_dims=matryoshka_dimensions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJG4fOm66PHI"
   },
   "source": [
    "##### ðŸ—ï¸ Activity #2:\n",
    "\n",
    "Both of these losses sound \"cool\", but what are they - exactly - under the hood?\n",
    "\n",
    "Why are these losses specifically doing? Please write a short summary of each loss.\n",
    "\n",
    "> NOTE: This is a course focused on AI Engineering and the application of AI - looking for a hint? Try pasting the code (linked above) into ChatGPT/Claude to write the summary!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKxRuXfH844c"
   },
   "source": [
    "Now we can set-up our evaluator.\n",
    "\n",
    "> NOTE: Due to the formatting of our dataset - this is all we have to do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "f0hAFwUyaHQG"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "\n",
    "corpus = val_dataset['corpus']\n",
    "queries = val_dataset['questions']\n",
    "relevant_docs = val_dataset['relevant_contexts']\n",
    "\n",
    "evaluator = InformationRetrievalEvaluator(queries, corpus, relevant_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYfap_ct8-bU"
   },
   "source": [
    "We'll train this model for 5 epochs, though you could increase this number if we had a significant amount more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "svZG0pBHiQr6"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxitWoNX9DwW"
   },
   "source": [
    "It's training time!\n",
    "\n",
    "> NOTE: We're manually defining a warm-up period here - this is just to provide a smooth ramp into our training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dummy/dummy/runs/umypgt7t?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x134aa8830>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(mode=\"disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 753,
     "referenced_widgets": [
      "0d3fc6edfdab4fe9aff8e805632eadad",
      "ebe8aa7a82124b57bce2d826c1dea0fa",
      "aa11c10b4234452d9430744ab89b59a2",
      "ca05cbcd72cb41d9856804ffb17b26cb",
      "cc2a33e9a7ac4c5699346fcbf53b7c95",
      "408f4dfce21a45cfad36047677ec8658",
      "ca5804644ef345c1b4f670fb7f088fe8",
      "2a872283afaa4a33a9bc3f9e57b3650b",
      "fe4d1052824c4ed29c38c5311087e650",
      "e283b1608c4d4266a03c57dc95aabb2e",
      "bfc4997e3bd94e66bebaa1ffdae1b99e"
     ]
    },
    "id": "aDhUHZY-iR09",
    "outputId": "6dbd9320-f7b9-46d6-b891-efdd12086631"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c82fdc70934c46b9491232b6322308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  6/160 41:00 < 26:18:51, 0.00 it/s, Epoch 0.31/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "warmup_steps = int(len(loader) * EPOCHS * 0.1)\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(loader, train_loss)],\n",
    "    epochs=EPOCHS,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path='finetuned_arctic_ft',\n",
    "    show_progress_bar=True,\n",
    "    evaluator=evaluator,\n",
    "    evaluation_steps=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "58699484312f460cb96ac44e3af14aa2",
      "d45a7d95e5b14a6f88d5798fec9c40a7",
      "b011a6ccf8e745c6be6f3a17b7d61dce",
      "1016780729b04ba489d488922173ddae",
      "9d34dac405fd40139d5dc04eecef55ed",
      "d6169577ffb341a69eb0175e301b6a44",
      "d4acc9a7aca04564bfaefe33de079394",
      "4e2c257232c3472697e03350de43cb30",
      "381c780ce17e4662981175400fb8a0f8",
      "878dc96f87dd45f389948a546db33e94",
      "006bf6f5ebaf400486a6b82610381db0",
      "9198dd0fa8f04aa7b75307aa2d513bfb",
      "59cd26ae53024fbd85431b6683cd119c",
      "4e7ccd97042c4fb9a201b6dc76762e04",
      "72ec09e2cbbf4788b1561abfcdd0819a",
      "fb1e19624fde4d3c8048ce00264d6056",
      "9638a0456e0c41b1b9b9757932f55c53",
      "18825dc83221412ab602830fc00db71b",
      "a6ea48a80c194d128959422368aa0e10",
      "9f4026c62c60493caa18c014ae414e65"
     ]
    },
    "id": "b3iwclvyRD8L",
    "outputId": "1471e984-9351-478c-de34-6eb32263fa30"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45935e9e3c33447f841fcd129b80768d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "_pn-Y6yjRoHk"
   },
   "outputs": [],
   "source": [
    "hf_username = \"llm-wizard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "e5c86e0e33264ed8b6325e44f9421653",
      "acf3991e5d644f468264f561b28b514a",
      "4a723c6a56e94309b2e3abe63f28aedc",
      "ef25768d3b0746b48c6d02e9bd151bf9",
      "cc1f0bc4a1a74568b9c74a7392a1568f",
      "95160a05de5b402b9bdb0bd2d099cd00",
      "cf376b0ea3544055b8867d7013526502",
      "869814ecd49e46f9a33dd53130e6953f",
      "cc7d460d3c5a4ac6a9b64929736d6598",
      "13e9bf583f6442d395334947379a280a",
      "1995b4d98fe044e38f1980d47033ca40"
     ]
    },
    "id": "Nqhf3zWa9AiJ",
    "outputId": "c601b2a8-f8e9-4d71-9c7b-8ea4999ff077"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.34G/1.34G [00:35<00:00, 38.0MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/llm-wizard/legal-ft-2/commit/fa3bc1d917e6c0beef1e06b3d31c4abd0899b25b'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(f\"{hf_username}/legal-ft-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6bo0zW5k9Poq"
   },
   "source": [
    "## Task 5: Evaluating our Retriever\n",
    "\n",
    "Now that we have fine-tuned our retriever - let's see if it's worthwhile!\n",
    "\n",
    "We'll start with some basic imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Vq-2oqU0wHFr"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jD0qrIh9X8f"
   },
   "source": [
    "Now we'll define a function that will help us evaluate our retrieval process.\n",
    "\n",
    "> NOTE: We're assuming 1 correct document in a \"hit\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0713_3cowX4q"
   },
   "outputs": [],
   "source": [
    "def evaluate_openai(\n",
    "    dataset,\n",
    "    embed_model,\n",
    "    top_k=5,\n",
    "    verbose=False,\n",
    "):\n",
    "  corpus = dataset['corpus']\n",
    "  questions = dataset['questions']\n",
    "  relevant_docs = dataset['relevant_contexts']\n",
    "  documents = [Document(page_content=content, metadata={\"id\": doc_id}) for doc_id, content in corpus.items()]\n",
    "  vectorstore = FAISS.from_documents(documents, embed_model)\n",
    "\n",
    "  retriever = vectorstore.as_retriever(search_kwargs={\"k\": top_k})\n",
    "\n",
    "  eval_results = []\n",
    "  for id, question in tqdm(questions.items()):\n",
    "    retrieved_nodes = retriever.invoke(question)\n",
    "    retrieved_ids = [node.metadata[\"id\"] for node in retrieved_nodes]\n",
    "    expected_id = relevant_docs[id][0]\n",
    "    is_hit = expected_id in retrieved_ids\n",
    "    eval_results.append({\"id\": id, \"question\": question, \"expected_id\": expected_id, \"is_hit\": is_hit})\n",
    "\n",
    "  return eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOr49m4O9lxY"
   },
   "source": [
    "All that's left to do is evaluate, we'll evaluate our model against:\n",
    "\n",
    "1. OpenAI's closed source `text-embedding-3-small`\n",
    "2. The base non-fine-tuned version of `Snowflake/snowflake-arctic-embed-l`.\n",
    "\n",
    "Let's see how it stacks up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijaeYpf593IW"
   },
   "source": [
    "### `text-embedding-3-small`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kyY3PztaxnU3",
    "outputId": "5a1ec5e9-00fc-4140-d5b6-aa752dd4c9fa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to multipart ingest runs: langsmith.utils.LangSmithRateLimitError: Rate limit exceeded for https://api.smith.langchain.com/runs/multipart. HTTPError('429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Too many requests: tenant exceeded usage limits: Monthly unique traces usage limit exceeded\"}\\n')trace=ffb182b5-f82d-4f1a-93d3-e2a41f73de72,id=ffb182b5-f82d-4f1a-93d3-e2a41f73de72\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithRateLimitError: Rate limit exceeded for https://api.smith.langchain.com/runs/multipart. HTTPError('429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Too many requests: tenant exceeded usage limits: Monthly unique traces usage limit exceeded\"}\\n')\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithRateLimitError: Rate limit exceeded for https://api.smith.langchain.com/runs/multipart. HTTPError('429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Too many requests: tenant exceeded usage limits: Monthly unique traces usage limit exceeded\"}\\n')\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithRateLimitError: Rate limit exceeded for https://api.smith.langchain.com/runs/multipart. HTTPError('429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Too many requests: tenant exceeded usage limits: Monthly unique traces usage limit exceeded\"}\\n')\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:04<00:00,  2.20it/s]\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithRateLimitError: Rate limit exceeded for https://api.smith.langchain.com/runs/multipart. HTTPError('429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Too many requests: tenant exceeded usage limits: Monthly unique traces usage limit exceeded\"}\\n')\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithRateLimitError: Rate limit exceeded for https://api.smith.langchain.com/runs/multipart. HTTPError('429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Too many requests: tenant exceeded usage limits: Monthly unique traces usage limit exceeded\"}\\n')\n"
     ]
    }
   ],
   "source": [
    "te3_openai = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "te3_results = evaluate_openai(test_dataset, te3_openai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "kkyW90TCxx_i"
   },
   "outputs": [],
   "source": [
    "te3_results_df = pd.DataFrame(te3_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MscVRdNCylJ-",
    "outputId": "275beff8-3c59-4063-8270-c01736b4ee05"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te3_hit_rate = te3_results_df[\"is_hit\"].mean()\n",
    "te3_hit_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Ra-mh0L96dQ"
   },
   "source": [
    "### `Snowflake/snowflake-arctic-embed-l` (base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OEskxwvFypHe",
    "outputId": "a3aad8ce-48ef-4d8f-9ed0-122b1ec9a000"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 10.55it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "huggingface_embeddings = HuggingFaceEmbeddings(model_name=\"Snowflake/snowflake-arctic-embed-l\")\n",
    "arctic_embed_m_results = evaluate_openai(test_dataset, huggingface_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "KlKgiXTWzMTg"
   },
   "outputs": [],
   "source": [
    "arctic_embed_m_results_df = pd.DataFrame(arctic_embed_m_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zV5vJWrJzOhc",
    "outputId": "30049be6-deb9-4ceb-dc13-24d7e125f131"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arctic_embed_m_hit_rate = arctic_embed_m_results_df[\"is_hit\"].mean()\n",
    "arctic_embed_m_hit_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcR3-0s19_lu"
   },
   "source": [
    "### `Snowflake/snowflake-arctic-embed-l` (fine-tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ilse1LduzP1i",
    "outputId": "292ab58f-7594-45fc-a5b8-1062cc553f66"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at llm-wizard/legal-ft-2 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "finetune_embeddings = HuggingFaceEmbeddings(model_name=\"llm-wizard/legal-ft-2\")\n",
    "# finetune_results = evaluate_openai(test_dataset, finetune_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "xxhZPqkNzZlh"
   },
   "outputs": [],
   "source": [
    "finetune_results_df = pd.DataFrame(finetune_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4thAK2BXzaj6",
    "outputId": "e890b5d1-86b7-4bfe-8ffe-a779a132e0c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.0)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_hit_rate = finetune_results_df[\"is_hit\"].mean()\n",
    "finetune_hit_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iegFM209mBk3"
   },
   "source": [
    "## Task 1: Vibe Checking the RAG Pipeline\n",
    "\n",
    "We're going to use our RAG pipeline to vibe check on some common phrases now that we've modified it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xzg0AA5krgR4"
   },
   "source": [
    "### Creating New Chunks\n",
    "\n",
    "In order to try and evaluate our system more fairly, let's create new chunks that we will use to create our Vector Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KwQ2_LqNr0Tw"
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 600,\n",
    "    chunk_overlap  = 50,\n",
    "    length_function = len\n",
    ")\n",
    "\n",
    "training_documents = text_splitter.split_documents(text_loader.load())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIdxahHXpP-c"
   },
   "source": [
    "### Base Chain\n",
    "\n",
    "We'll start by constructing our base chain, which will use the untrained retrieval model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOsxIXpNpWC2"
   },
   "source": [
    "#### R - Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "azIGIKYfmNCT"
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "base_vectorstore = FAISS.from_documents(training_documents, huggingface_embeddings)\n",
    "base_retriever = base_vectorstore.as_retriever(search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-1nVZ0KpX5N"
   },
   "source": [
    "#### A - Augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "G10Fr-aKojeA"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "RAG_PROMPT = \"\"\"\\\n",
    "Given a provided context and a question, you must answer the question. If you do not know the answer, you must state that you do not know.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt_template = ChatPromptTemplate.from_template(RAG_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Euq6RQEopZvD"
   },
   "source": [
    "#### G - Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5-mfbbrypMHG"
   },
   "outputs": [],
   "source": [
    "rag_llm =  ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQ2p4mnUpbYY"
   },
   "source": [
    "#### RAG - LCEL RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ssuR-LaboyGq"
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "\n",
    "base_rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | base_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt_template | rag_llm | StrOutputParser(), \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "emm6WbB9pfKt",
    "outputId": "f0e0c83f-c617-493e-99ce-869061a315f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An agent, in the context of AI, is an infuriatingly vague term that generally refers to AI systems that can act on your behalf. There are two main interpretations: one sees agents as entities that go and perform tasks for you (like a travel agent), while the other views them as LLMs (large language models) that have access to tools and can run processes in a loop to solve problems. However, the term lacks a clear and widely understood definition, leading to confusion about its meaning and utility.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_rag_chain.invoke({\"question\" : \"What is an agent?\"})[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "mUOrd0OBprAq",
    "outputId": "0070d677-0bde-48be-a8a3-f0c53b9eee90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Organizations that have produced better-than-GPT-3 class models include Anthropic, Mistral, Google, Meta, EleutherAI, Stability AI, TII in Abu Dhabi (Falcon), Microsoft Research, xAI, Replit, Baidu, and several others.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_rag_chain.invoke({\"question\" : \"Who has produced better models than GPT-3?\"})[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "OnfuFl59py7I",
    "outputId": "37480e29-17a6-40e2-fe8a-7eb6ea270569"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I do not know.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_rag_chain.invoke({\"question\" : \"What is the laziest AI month?\"})[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "-NmqwHBDqTZ8",
    "outputId": "86d7b59a-97c6-4b65-805f-ae449e3b1a20"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I do not know.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_rag_chain.invoke({\"question\" : \"What is the largest model that Simon has run on his phone?\"})[\"response\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqNS0UJAp3lC"
   },
   "source": [
    "### Fine-tuned Embedding Model\n",
    "\n",
    "Now let's rebuild our RAG chain with the Fine-tuned model - the only component we need to change is our `FAISS` vectorstore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ihO7tP6mqATy"
   },
   "outputs": [],
   "source": [
    "finetune_vectorstore = FAISS.from_documents(training_documents, finetune_embeddings)\n",
    "finetune_retriever = finetune_vectorstore.as_retriever(search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "1_cIFvWzqKGY"
   },
   "outputs": [],
   "source": [
    "finetune_rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | finetune_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt_template | rag_llm | StrOutputParser(), \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "OJmRHJF2qNgj",
    "outputId": "b99d4b58-8487-48ec-ee6c-8ea93f9b0192"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An \"agent\" is a term that lacks a single, clear, and widely understood meaning in the context of AI. It generally refers to AI systems that can act on behalf of a user, but there are various interpretations of what this entails. Some people view agents as systems that go and act on your behalf, similar to a travel agent, while others think of them as LLMs (Large Language Models) that have access to tools and can run tasks in a loop to solve problems. The term is often associated with concepts of autonomy, but there is significant ambiguity and skepticism surrounding their utility, particularly due to issues like gullibility, where AI systems may believe and act on false information.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_rag_chain.invoke({\"question\" : \"What is an Agent?\"})[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "EnK-c2ugqPPh",
    "outputId": "b8300e0a-1b51-48dd-93c3-254e0aa84e36"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Organizations that have produced better-than-GPT-3 class models include Anthropic, Mistral, Google, Meta, EleutherAI, Stability AI, TII in Abu Dhabi (Falcon), Microsoft Research, xAI, Replit, Baidu, and several others.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_rag_chain.invoke({\"question\" : \"Who has produced better models than GPT-3?\"})[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "83hssg1AWozc",
    "outputId": "8d1ef134-7889-4379-a49b-6c0a476b7a1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The laziest AI month is suggested to be December, as the context mentions that ChatGPT may get lazy in December due to its hidden system prompt including the current date and the observation that people provide less useful answers as the holidays approach.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_rag_chain.invoke({\"question\" : \"What is the laziest AI month?\"})[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "rsHmGeFbqRET",
    "outputId": "1ab223c2-d2fc-46ac-8541-af038de3166d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Simon has run the Mistral 7B model on his iPhone.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_rag_chain.invoke({\"question\" : \"What is the largest model that Simon has run on his phone?\"})[\"response\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDgD8seY_I3W"
   },
   "source": [
    "####â“Question #2:\n",
    "\n",
    "Which LCEL RAG Chain do you think answered the questions better, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCbq1sZArIx4"
   },
   "source": [
    "## Task 2: RAGAS Evaluation\n",
    "\n",
    "It's great to have some idea of how our system is doing based on vibe-checks, but let's use RAGAS to provide more insight info. on how things are improving!\n",
    "\n",
    "> NOTE: Please recreate *exactly* the RAGAS process we used to evaluate RAG, baselining with the default retriever, and then comparing the new retriever. The includes the Synthetic Data Generation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "jq880DtHk9pX"
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 750,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = len\n",
    ")\n",
    "text_loader = DirectoryLoader(\"data/\", glob=\"*.html\", loader_cls=BSHTMLLoader)\n",
    "\n",
    "docs_as_chunks = text_splitter.split_documents(text_loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_as_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "176cb5cc86f94c29bd0ef462d5b16b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying SummaryExtractor:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "609c709d5e64420cbb596b99f40ede41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying CustomNodeFilter:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Node e07d4965-5af7-45bb-9e0c-2c051bce2afe does not have a summary. Skipping filtering.\n",
      "Node 5810a9b7-87e3-4884-bde7-24c32dbbc80c does not have a summary. Skipping filtering.\n",
      "Node 07069017-8302-4c8c-81e8-cf7bdc23a79f does not have a summary. Skipping filtering.\n",
      "Node cdbd1cf5-47c0-489b-8b69-d9890196d7c2 does not have a summary. Skipping filtering.\n",
      "Node 94036fe2-5eb2-4b1d-a9e3-ff5c62d98279 does not have a summary. Skipping filtering.\n",
      "Node 8f67a02f-648f-499a-99f4-ca0f809dd07d does not have a summary. Skipping filtering.\n",
      "Node 8c8b80bc-12bb-482a-9ec9-178bc76465c0 does not have a summary. Skipping filtering.\n",
      "Node 6ddadbd4-66c0-49e4-8240-1c67b21a3663 does not have a summary. Skipping filtering.\n",
      "Node ed802596-ba5f-4665-a5cb-174f377bf233 does not have a summary. Skipping filtering.\n",
      "Node edd8fbcc-cdf7-46de-a45f-3b76ebe7e7a8 does not have a summary. Skipping filtering.\n",
      "Node ab2acd5a-ad5c-4a6c-8dd5-188eba5f2572 does not have a summary. Skipping filtering.\n",
      "Node c0028115-ebd6-4dfc-8123-ed3d4ffab5f4 does not have a summary. Skipping filtering.\n",
      "Node 04bc66f0-a228-40fc-a86b-c9ab2a329034 does not have a summary. Skipping filtering.\n",
      "Node efe61adf-931a-4f16-9256-f6060002e723 does not have a summary. Skipping filtering.\n",
      "Node 15d00082-f18b-41dd-9d11-eac134234acd does not have a summary. Skipping filtering.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb2b50d21ed4d279b6705b906a3ae0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-6PX9ZovnaTEDbiXm0A2pvlFW on tokens per min (TPM): Limit 30000, Used 29870, Requested 475. Please try again in 690ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "793aeed3376f45309549ee12f8cfd6dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying OverlapScoreBuilder:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "768478da425e40898e9514a1ee0764aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cfc3ec9fc634aa195a428260ffeec9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Scenarios:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6610840e604bf1a3bd20738937a3ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Samples:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "\n",
    "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
    "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
    "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
    "dataset = generator.generate_with_langchain_docs(docs_as_chunks, testset_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate testset that contains: \n",
    "# - question\n",
    "# - ragas context\n",
    "# - ragas response\n",
    "# - actual context\n",
    "# - actual response \n",
    "# for base model (no fine tuning)\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "base_model_dataset = deepcopy(dataset)\n",
    "\n",
    "for test_row in base_model_dataset:\n",
    "  response = await base_rag_chain.ainvoke({\"question\" : test_row.eval_sample.user_input})\n",
    "  test_row.eval_sample.response = response[\"response\"]\n",
    "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate testset that contains: \n",
    "# - question\n",
    "# - ragas context\n",
    "# - ragas response\n",
    "# - actual context\n",
    "# - actual response \n",
    "# for fine tuned model\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "finetune_model_dataset = deepcopy(dataset)\n",
    "\n",
    "for test_row in finetune_model_dataset:\n",
    "  response = await finetune_rag_chain.ainvoke({\"question\" : test_row.eval_sample.user_input})\n",
    "  test_row.eval_sample.response = response[\"response\"]\n",
    "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82a250fe2d64481a8419e9d42d09e2d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41dff0a164c24098bd6251bb5fee46e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ragas import EvaluationDataset\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, ResponseRelevancy, ContextEntityRecall, NoiseSensitivity\n",
    "from ragas import evaluate, RunConfig\n",
    "\n",
    "base_eval_dataset = EvaluationDataset.from_pandas(base_model_dataset.to_pandas())\n",
    "finetune_eval_dataset = EvaluationDataset.from_pandas(finetune_model_dataset.to_pandas())\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "custom_run_config = RunConfig(timeout=360)\n",
    "\n",
    "base_result = evaluate(\n",
    "    dataset=base_eval_dataset,\n",
    "    metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness(), ResponseRelevancy(), ContextEntityRecall(), NoiseSensitivity()],\n",
    "    llm=evaluator_llm,\n",
    "    run_config=custom_run_config\n",
    ")\n",
    "\n",
    "finetune_result = evaluate(\n",
    "    dataset=finetune_eval_dataset,\n",
    "    metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness(), ResponseRelevancy(), ContextEntityRecall(), NoiseSensitivity()],\n",
    "    llm=evaluator_llm,\n",
    "    run_config=custom_run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compare(x, y, x_name, y_name) -> None:\n",
    "    dict1 = {k: round(v, 4) for k, v in x._repr_dict.items()}\n",
    "    dict2 = {k: round(v, 4) for k, v in y._repr_dict.items()}\n",
    "\n",
    "    assert dict1.keys() == dict2.keys()\n",
    "\n",
    "    metrics = list(dict1.keys())\n",
    "    values1 = list(dict1.values())\n",
    "    values2 = list(dict2.values())\n",
    "\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(x - width / 2, values1, width, label=x_name, color=\"blue\", alpha=0.7)\n",
    "    ax.bar(x + width / 2, values2, width, label=y_name, color=\"orange\", alpha=0.7)\n",
    "\n",
    "    ax.set_xlabel(\"Metrics\")\n",
    "    ax.set_ylabel(\"Values\")\n",
    "    ax.set_title(\"Comparison of Models\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metrics)\n",
    "    ax.legend()\n",
    "\n",
    "    plt.ylim(0, 1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "compare(base_result, finetune_result, \"Base model\", \"Fine-tuned model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "006bf6f5ebaf400486a6b82610381db0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0d3fc6edfdab4fe9aff8e805632eadad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ebe8aa7a82124b57bce2d826c1dea0fa",
       "IPY_MODEL_aa11c10b4234452d9430744ab89b59a2",
       "IPY_MODEL_ca05cbcd72cb41d9856804ffb17b26cb"
      ],
      "layout": "IPY_MODEL_cc2a33e9a7ac4c5699346fcbf53b7c95"
     }
    },
    "1016780729b04ba489d488922173ddae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "CheckboxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_tooltip": null,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_9198dd0fa8f04aa7b75307aa2d513bfb",
      "style": "IPY_MODEL_59cd26ae53024fbd85431b6683cd119c",
      "value": true
     }
    },
    "13e9bf583f6442d395334947379a280a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "18825dc83221412ab602830fc00db71b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a6ea48a80c194d128959422368aa0e10",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_9f4026c62c60493caa18c014ae414e65",
      "value": "Connecting..."
     }
    },
    "1995b4d98fe044e38f1980d47033ca40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2a872283afaa4a33a9bc3f9e57b3650b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "381c780ce17e4662981175400fb8a0f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "408f4dfce21a45cfad36047677ec8658": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a723c6a56e94309b2e3abe63f28aedc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_869814ecd49e46f9a33dd53130e6953f",
      "max": 1336413848,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cc7d460d3c5a4ac6a9b64929736d6598",
      "value": 1336413848
     }
    },
    "4e2c257232c3472697e03350de43cb30": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4e7ccd97042c4fb9a201b6dc76762e04": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "58699484312f460cb96ac44e3af14aa2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [],
      "layout": "IPY_MODEL_d4acc9a7aca04564bfaefe33de079394"
     }
    },
    "59cd26ae53024fbd85431b6683cd119c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "72ec09e2cbbf4788b1561abfcdd0819a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "869814ecd49e46f9a33dd53130e6953f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "878dc96f87dd45f389948a546db33e94": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9198dd0fa8f04aa7b75307aa2d513bfb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95160a05de5b402b9bdb0bd2d099cd00": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9638a0456e0c41b1b9b9757932f55c53": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9d34dac405fd40139d5dc04eecef55ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_4e7ccd97042c4fb9a201b6dc76762e04",
      "style": "IPY_MODEL_72ec09e2cbbf4788b1561abfcdd0819a",
      "tooltip": ""
     }
    },
    "9f4026c62c60493caa18c014ae414e65": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a6ea48a80c194d128959422368aa0e10": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa11c10b4234452d9430744ab89b59a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2a872283afaa4a33a9bc3f9e57b3650b",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fe4d1052824c4ed29c38c5311087e650",
      "value": 1
     }
    },
    "acf3991e5d644f468264f561b28b514a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_95160a05de5b402b9bdb0bd2d099cd00",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_cf376b0ea3544055b8867d7013526502",
      "value": "model.safetensors:â€‡100%"
     }
    },
    "b011a6ccf8e745c6be6f3a17b7d61dce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_878dc96f87dd45f389948a546db33e94",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_006bf6f5ebaf400486a6b82610381db0",
      "value": ""
     }
    },
    "bfc4997e3bd94e66bebaa1ffdae1b99e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ca05cbcd72cb41d9856804ffb17b26cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e283b1608c4d4266a03c57dc95aabb2e",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_bfc4997e3bd94e66bebaa1ffdae1b99e",
      "value": "â€‡0/1â€‡[00:00&lt;?,â€‡?example/s]"
     }
    },
    "ca5804644ef345c1b4f670fb7f088fe8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cc1f0bc4a1a74568b9c74a7392a1568f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc2a33e9a7ac4c5699346fcbf53b7c95": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "cc7d460d3c5a4ac6a9b64929736d6598": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cf376b0ea3544055b8867d7013526502": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d45a7d95e5b14a6f88d5798fec9c40a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4e2c257232c3472697e03350de43cb30",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_381c780ce17e4662981175400fb8a0f8",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "d4acc9a7aca04564bfaefe33de079394": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "d6169577ffb341a69eb0175e301b6a44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb1e19624fde4d3c8048ce00264d6056",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_9638a0456e0c41b1b9b9757932f55c53",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "e283b1608c4d4266a03c57dc95aabb2e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5c86e0e33264ed8b6325e44f9421653": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_acf3991e5d644f468264f561b28b514a",
       "IPY_MODEL_4a723c6a56e94309b2e3abe63f28aedc",
       "IPY_MODEL_ef25768d3b0746b48c6d02e9bd151bf9"
      ],
      "layout": "IPY_MODEL_cc1f0bc4a1a74568b9c74a7392a1568f"
     }
    },
    "ebe8aa7a82124b57bce2d826c1dea0fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_408f4dfce21a45cfad36047677ec8658",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_ca5804644ef345c1b4f670fb7f088fe8",
      "value": "Computingâ€‡widgetâ€‡examples:â€‡â€‡â€‡0%"
     }
    },
    "ef25768d3b0746b48c6d02e9bd151bf9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_13e9bf583f6442d395334947379a280a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_1995b4d98fe044e38f1980d47033ca40",
      "value": "â€‡1.34G/1.34Gâ€‡[01:11&lt;00:00,â€‡21.0MB/s]"
     }
    },
    "fb1e19624fde4d3c8048ce00264d6056": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe4d1052824c4ed29c38c5311087e650": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
